<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/lru/ssm.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/lru/ssm.py" />
              <option name="originalContent" value="import math&#10;from dataclasses import dataclass&#10;import torch&#10;import torch.nn as nn&#10;from collections import OrderedDict&#10;from lru.scan_utils import associative_scan, binary_operator_diag&#10;import torch.jit as jit&#10;from lru.L_bounded_MLPs import FirstChannel, SandwichFc, SandwichLin&#10;from typing import Optional&#10;&#10;&#10;&quot;&quot;&quot; Linear Recurrent Units ----------------------------------------- &quot;&quot;&quot;&#10;&#10;class LRU(nn.Module):&#10;    &quot;&quot;&quot; Linear Recurrent Unit. The LRU is simulated using Parallel Scan (fast!) when&#10;     &quot;scan&quot; is set to True (default) in the forward pass, otherwise recursively (slow).&quot;&quot;&quot;&#10;&#10;    def __init__(&#10;            self, in_features: int, out_features: int, state_features: int, internal_state_init=None, rmin=0.9,&#10;            rmax=1.0, max_phase=6.283&#10;    ):&#10;        super().__init__()&#10;        self.in_features = in_features&#10;        self.out_features = out_features&#10;        self.state_features = state_features&#10;&#10;        # Pre-compute constants for efficiency&#10;        self._sqrt_in_features = math.sqrt(in_features)&#10;        self._sqrt_2_in_features = math.sqrt(2 * in_features)&#10;        self._sqrt_state_features = math.sqrt(state_features)&#10;        self._rmin_rmax_diff = rmax - rmin&#10;        self._rmin_rmax_sum = rmax + rmin&#10;        self._rmin_squared = rmin ** 2&#10;&#10;        self.D = nn.Parameter(&#10;            torch.randn([out_features, in_features]) / self._sqrt_in_features&#10;        )&#10;&#10;        u1 = torch.rand(state_features)&#10;        u2 = torch.rand(state_features)&#10;        self.nu_log = nn.Parameter(&#10;            torch.log(-0.5 * torch.log(u1 * self._rmin_rmax_sum * self._rmin_rmax_diff + self._rmin_squared))&#10;        )&#10;        self.theta_log = nn.Parameter(torch.log(max_phase * u2))&#10;&#10;        lambda_abs = torch.exp(-torch.exp(self.nu_log))&#10;        self.gamma_log = nn.Parameter(&#10;            torch.log(torch.sqrt(1.0 - lambda_abs.square()))  # More efficient than torch.ones_like and torch.square&#10;        )&#10;&#10;        # More efficient initialization using a single complex tensor creation&#10;        B_complex = torch.complex(&#10;            torch.randn([state_features, in_features]) / self._sqrt_2_in_features,&#10;            torch.randn([state_features, in_features]) / self._sqrt_2_in_features&#10;        )&#10;        self.B = nn.Parameter(B_complex)  # N, U&#10;&#10;        C_complex = torch.complex(&#10;            torch.randn([out_features, state_features]) / self._sqrt_state_features,&#10;            torch.randn([out_features, state_features]) / self._sqrt_state_features&#10;        )&#10;        self.C = nn.Parameter(C_complex)  # H, N&#10;&#10;        # initialize internal state&#10;        self.state = None&#10;&#10;        # Pre-compute transformation matrices for ss_real_matrices method&#10;        self._T_block = None&#10;        self._T_block_inv = None&#10;&#10;    def ss_params(self):&#10;        lambda_abs = torch.exp(-torch.exp(self.nu_log))&#10;        lambda_phase = torch.exp(self.theta_log)&#10;&#10;        # More efficient complex number creation&#10;        lambdas = lambda_abs * torch.exp(1j * lambda_phase)&#10;        gammas = torch.exp(self.gamma_log).unsqueeze(-1)&#10;        B = gammas * self.B&#10;        return lambdas, B, self.C, self.D&#10;&#10;    def ss_real_matrices(self, to_numpy=True):&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # Pre-allocate with the correct dtype and device&#10;        device, dtype = lambdas.device, lambdas.dtype&#10;        state_features_2 = 2 * self.state_features&#10;&#10;        # More efficient tensor creation using stack instead of manual indexing&#10;        lambdas_conjugate = torch.stack([lambdas, lambdas.conj()], dim=1).flatten()&#10;        A_full = torch.diag(lambdas_conjugate)&#10;&#10;        # More efficient B_full creation&#10;        B_conjugate = torch.stack([B, B.conj()], dim=1).view(state_features_2, self.in_features)&#10;&#10;        # More efficient C_full creation&#10;        C_half = 0.5 * C&#10;        C_conjugate = torch.stack([C_half, C_half.conj()], dim=2).view(self.out_features, state_features_2)&#10;&#10;        # Cache transformation matrices&#10;        if self._T_block is None or self._T_block.device != device:&#10;            self._T_block = torch.tensor([[1, 1], [1j, -1j]], device=device, dtype=dtype)&#10;            self._T_block_inv = torch.linalg.inv(self._T_block)&#10;&#10;        T_full = torch.block_diag(*([self._T_block] * self.state_features))&#10;        T_full_inv = torch.block_diag(*([self._T_block_inv] * self.state_features))&#10;&#10;        # More efficient matrix operations using @ operator consistently&#10;        A_real = (T_full @ A_full @ T_full_inv).real&#10;        B_real = (T_full @ B_conjugate).real&#10;        C_real = (C_conjugate @ T_full_inv).real&#10;        D_real = D&#10;&#10;        ss_real_params = [A_real, B_real, C_real, D_real]&#10;        if to_numpy:&#10;            ss_real_params = [param.detach().cpu().numpy() for param in ss_real_params]&#10;&#10;        return tuple(ss_real_params)&#10;&#10;    def forward_loop(self, input, state=None):&#10;        batch_size, seq_len, _ = input.shape&#10;&#10;        # State management&#10;        if self.state is None or self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                     device=input.device, dtype=torch.complex64)&#10;&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # State computation using pre-converted input&#10;        input_B_dtype = input.to(B.dtype)&#10;        B_T = B.mT  # Cache transpose&#10;&#10;        # Optimized loop with pre-allocated tensor for states&#10;        inner_states = torch.empty(batch_size, seq_len, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        # Vectorized state updates&#10;        current_state = self.state&#10;        for t, u_step in enumerate(input_B_dtype.unbind(dim=1)):&#10;            inner_states[:, t] = current_state&#10;            current_state = lambdas * current_state + u_step @ B_T&#10;&#10;        self.state = current_state  # Update the internal state&#10;&#10;        # Output computation using all inner states&#10;        output = (inner_states @ C.mT).real + input @ D.T&#10;&#10;        return output, inner_states&#10;&#10;    @torch.compiler.disable&#10;    def forward_scan(self, input, state=None):&#10;        &quot;&quot;&quot;&#10;        Computes the LRU output using a parallel scan.&#10;&#10;        Args:&#10;            input (torch.Tensor): (B, L, H) input sequence.&#10;            state (torch.Tensor, optional): (B, N) initial state. If None, a zero state is used.&#10;&#10;        Returns:&#10;            Tuple[torch.Tensor, torch.Tensor]:&#10;                - (B, L, H_out) output sequence.&#10;                - (B, L, N) sequence of internal states.&#10;        &quot;&quot;&quot;&#10;        batch_size, seq_len, _ = input.shape&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # If no initial state is provided, initialize it to zeros.&#10;        if self.state is None or self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        # Pre-compute input transformation&#10;        Bu_elements = input.to(B.dtype) @ B.mT&#10;&#10;        # Incorporate the initial state into the first element of the sequence&#10;        Bu_elements[:, 0, :] += lambdas * self.state&#10;&#10;        # Define the scan function for vmap&#10;        lambda_elements = lambdas.expand(seq_len, -1)&#10;&#10;        def scan_fn(Bu_seq):&#10;            return associative_scan(binary_operator_diag, (lambda_elements, Bu_seq))[1]&#10;&#10;        # Apply the scan over the batch dimension&#10;        scanned_states = torch.vmap(scan_fn)(Bu_elements)&#10;&#10;        # Prepend the initial state to get the full state sequence&#10;        inner_states = torch.cat([self.state.unsqueeze(1), scanned_states[:, :-1, :]], dim=1)&#10;&#10;        # Update the internal state of the LRU module to the last state&#10;        self.state = scanned_states[:, -1, :]&#10;&#10;        # Compute the final output&#10;        output = (inner_states @ C.mT).real + input @ D.T&#10;        return output, inner_states&#10;&#10;    def forward(self, input, gamma=None, state=None, mode=&quot;loop&quot;):&#10;        if mode == &quot;scan&quot;:&#10;            return self.forward_scan(input, self.state)&#10;        elif mode in [&quot;loop&quot;, &quot;loop_efficient&quot;]:&#10;            return self.forward_loop(input, self.state)&#10;        else:&#10;            raise ValueError(f&quot;Unknown mode: {mode}. Expected 'scan', 'loop', or 'loop_efficient'.&quot;)&#10;&#10;&#10;    def reset(self):&#10;        self.state = None  # reset the SSM state to the initial value&#10;&#10;&#10;# WORK IN PROGRESS&#10;&#10;class LRU_Robust(jit.ScriptModule):&#10;    &quot;&quot;&quot; Implements a Linear Recurrent Unit (LRU) with trainable or prescribed l2 gain gamma.&#10;    No parallel scan implementation is available at the moment. &quot;&quot;&quot;&#10;&#10;    def __init__(self, state_features: int, gamma: float):&#10;        super().__init__()&#10;        self.state_features = state_features&#10;        self.gamma = gamma&#10;        if gamma is None:&#10;            self.gamma = nn.Parameter(torch.tensor(33.0))&#10;        # initialize the internal state with TorchScript-friendly Optional[Tensor]&#10;        self.state = torch.zeros(1)&#10;        self.register_buffer('ID', torch.eye(state_features))&#10;&#10;        self.alpha = nn.Parameter(torch.tensor(4.1))  # controls the initialization of the matrix A:&#10;        # the larger the alpha at initialization, the closer the eigenvalues of A will be&#10;        # to the boundary of the unitary circle at initialization. This helps the SSM to obtain long memory properties.&#10;&#10;        self.epsilon = nn.Parameter(torch.tensor(-99.9))  # Regularization&#10;&#10;        self.Skew = nn.Parameter(0.01 * torch.randn(state_features, state_features))&#10;&#10;        # Define each block of X as a parameter&#10;        self.X11 = nn.Parameter(torch.eye(state_features))&#10;        self.X22 = nn.Parameter(0.01 * torch.eye(state_features))&#10;        self.X21 = nn.Parameter(torch.eye(state_features))&#10;&#10;        self.C = nn.Parameter(torch.eye(state_features))&#10;        self.D = nn.Parameter(torch.eye(state_features))&#10;&#10;        # self.X11 = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.X22 = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.X21 = nn.Parameter(torch.randn(state_features, state_features))&#10;        #&#10;        # self.C = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.D = nn.Parameter(torch.randn(state_features, state_features))&#10;&#10;    @jit.script_method&#10;    def set_param(self):  # Parameter update for l2 gain (free param)&#10;&#10;        gamma = self.gamma&#10;        # Auxiliary Parameters&#10;        X11 = self.X11&#10;        X22 = self.X22&#10;&#10;        Sk = self.Skew - self.Skew.T&#10;        Q = (self.ID - Sk) @ torch.linalg.inv(self.ID + Sk)&#10;        Z = self.X21 @ self.X21.T + X22 @ X22.T + self.D.T @ self.D + torch.exp(self.epsilon) * self.ID&#10;        beta = gamma ** 2 * torch.sigmoid(self.alpha) / torch.norm(Z, 2)&#10;        H11 = X11 @ X11.T + self.C.T @ self.C + beta * torch.exp(self.epsilon) * self.ID&#10;        H12 = torch.sqrt(beta) * (X11 @ self.X21.T + self.C.T @ self.D)&#10;        V = Z * beta - gamma ** 2 * self.ID&#10;        R = H12 @ torch.linalg.inv(V.T) @ H12.T&#10;        CR = torch.linalg.cholesky(-R)&#10;        CRH = torch.linalg.cholesky(-R + H11)&#10;&#10;        # LTI system matrices&#10;&#10;        A = torch.linalg.inv(CRH).T @ Q @ CR.T&#10;        B = A @ torch.linalg.inv(H12.T) @ V.T&#10;        C = self.C&#10;        D = torch.sqrt(beta) * self.D&#10;&#10;        return A, B, C, D&#10;&#10;    #state: Optional[torch.Tensor]&#10;    @jit.script_method&#10;    def forward(self, input: torch.Tensor, mode: str = &quot;loop&quot;) -&gt; tuple[torch.Tensor, torch.Tensor]:&#10;        # Input size: (B, L, H)&#10;        batch_size = input.shape[0]&#10;&#10;        # Initialize state if needed&#10;        if self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        A, B, C, D = self.set_param()&#10;        if input.dim() == 1:&#10;            input = input.unsqueeze(0).unsqueeze(0)&#10;        # output = torch.empty(&#10;        #     [i for i in input.shape[:-1]] + [self.state_features], device=self.C.device&#10;        # )&#10;&#10;        states = []&#10;&#10;        # LTI dynamics&#10;        for u_step in input.split(1, dim=1):  # 1 is the time dimension&#10;            u_step = u_step.squeeze(1)&#10;            self.state = self.state @ A.T + u_step @ B.T&#10;            states.append(self.state)&#10;&#10;        states = torch.stack(states, 1)&#10;        output = states @ C.mT + input @ D.T&#10;        return output, states&#10;&#10;    def reset(self):&#10;        self.state = None  # reset the LRU state to the initial value&#10;&#10;&quot;&quot;&quot; SSM models ----------------------------------------- &quot;&quot;&quot;&#10;&#10;&quot;&quot;&quot; Data class to set up the SSM model (values here are used just to initialize all fields) &quot;&quot;&quot;&#10;@dataclass&#10;class SSMConfig:&#10;    d_model: int = 10  # input/output size of the LRU (n_u = n_y)&#10;    d_state: int = 64  # state size of the LRU (n_x)&#10;    n_layers: int = 6  # number of SSMs blocks in cascade for deep structures&#10;    dropout: float = 0.0  # set it different from 0 if you want to introduce dropout regularization&#10;    bias: bool = False  # bias of MLP layers&#10;    rmin: float = 0.0  # min. magnitude of the eigenvalues at initialization in the complex parametrization&#10;    rmax: float = 1.0  # max. magnitude of the eigenvalues at initialization in the complex parametrization&#10;    max_phase: float = 2 * math.pi  # maximum phase of the eigenvalues at initialization in the complex parametrization&#10;    ff: str = &quot;MLP&quot;  # non-linear block used in the scaffolding&#10;    scale: float = 1  # Lipschitz constant of the Lipschitz bounded MLP (LMLP)&#10;    dim_amp: int = 4  # controls the hidden layer's dimension of the MLP&#10;    robust: bool = True  # set this to true if you want to use the l2 gain parametrization for the SSM. If set to false,&#10;    # the complex diagonal parametrization of the LRU will be used instead.&#10;    gamma: float = None  # set the overall l2 gain value in case you want to keep it fixed and not trainable, if set to&#10;    # None, the gain will be trainable.&#10;&#10;&#10;    # Parallel scan must be selected in the forward call. It will be disabled when gamma is set to True.&#10;&#10;    &quot;&quot;&quot; Scaffolding Layers &quot;&quot;&quot;&#10;&#10;class MLP(nn.Module):&#10;    &quot;&quot;&quot; Standard Transformer MLP &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        # Pre-compute hidden dimension for efficiency&#10;        self.hidden_dim = config.dim_amp * config.d_model&#10;&#10;        self.c_fc = nn.Linear(config.d_model, self.hidden_dim, bias=False)&#10;        self.gelu = nn.GELU()&#10;        self.c_proj = nn.Linear(self.hidden_dim, config.d_model, bias=False)&#10;        self.dropout = nn.Dropout(config.dropout) if config.dropout &gt; 0 else nn.Identity()&#10;&#10;    def forward(self, x):&#10;        x = self.c_fc(x)&#10;        x = self.gelu(x)&#10;        x = self.c_proj(x)&#10;        return self.dropout(x)&#10;&#10;&#10;class LMLP(nn.Module):&#10;    &quot;&quot;&quot; Implements a Lipschitz.-bounded MLP with sandwich layers. The square root&#10;    # of the Lipschitz bound is given by the scale parameter &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        # Pre-compute hidden dimension for efficiency&#10;        hidden_dim = config.dim_amp * config.d_model&#10;&#10;        # More efficient layer construction using list comprehension&#10;        layers = [&#10;            FirstChannel(config.d_model, scale=config.scale),&#10;            SandwichFc(config.d_model, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichFc(hidden_dim, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichFc(hidden_dim, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichLin(hidden_dim, config.d_model, bias=False, scale=config.scale)&#10;        ]&#10;&#10;        # Only add dropout if needed&#10;        if config.dropout &gt; 0:&#10;            layers.append(nn.Dropout(config.dropout))&#10;&#10;        self.model = nn.Sequential(*layers)&#10;&#10;    def forward(self, input):&#10;        return self.model(input)&#10;&#10;&#10;class GLU(nn.Module):&#10;    &quot;&quot;&quot; The static non-linearity used in the S4 paper &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        self.activation = nn.GELU()&#10;        self.dropout = nn.Dropout(config.dropout) if config.dropout &gt; 0 else nn.Identity()&#10;&#10;        # More efficient sequential construction&#10;        self.output_linear = nn.Sequential(&#10;            nn.Linear(config.d_model, 2 * config.d_model),&#10;            nn.GLU(dim=-1),&#10;        )&#10;&#10;    def forward(self, x):&#10;        x = self.dropout(self.activation(x))&#10;        return self.output_linear(x)&#10;&#10;    &quot;&quot;&quot; SSMs blocks ----------------------------------------- &quot;&quot;&quot;&#10;&#10;class SSL(nn.Module):&#10;    &quot;&quot;&quot; State Space Layer: LRU --&gt; MLP + skip connection &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        self.ln = nn.LayerNorm(config.d_model, bias=config.bias)&#10;&#10;        # More efficient LRU initialization&#10;        if config.robust:&#10;            self.lru = LRU_Robust(config.d_model, config.gamma)&#10;        else:&#10;            self.lru = LRU(config.d_model, config.d_model, config.d_state,&#10;                           rmin=config.rmin, rmax=config.rmax, max_phase=config.max_phase)&#10;&#10;        # Dictionary for layer selection&#10;        ff_layers = {&#10;            &quot;GLU&quot;: lambda: GLU(config),&#10;            &quot;MLP&quot;: lambda: MLP(config),&#10;            &quot;LMLP&quot;: lambda: LMLP(config)&#10;        }&#10;&#10;        if config.ff not in ff_layers:&#10;            raise ValueError(f&quot;Unknown feedforward type: {config.ff}&quot;)&#10;&#10;        self.ff = ff_layers[config.ff]()&#10;        self.dropout = nn.Dropout(config.dropout)&#10;&#10;    def forward(self, x, state=None, mode: str = &quot;loop&quot;):&#10;        z = x&#10;        # z = self.ln(z)  # prenorm&#10;&#10;        z, st = self.lru(z, state=state, mode=mode)&#10;        z = self.ff(z)  # MLP, GLU or LMLP&#10;        z = self.dropout(z)&#10;&#10;        # Residual connection&#10;        return z + x, st&#10;&#10;&#10;class DeepSSM(nn.Module):&#10;    &quot;&quot;&quot; Deep SSM: encoder --&gt; cascade of n SSM blocks --&gt; decoder  &quot;&quot;&quot;&#10;&#10;    def __init__(self, n_u: int, n_y: int, config: SSMConfig):&#10;        super().__init__()&#10;&#10;        self.config = config&#10;&#10;        # Simplified initialization - only handle trainable gamma for LRU_Robust&#10;        if config.robust is True and config.gamma is not None:&#10;            # Fixed-γ: register buffer and use raw Parameters&#10;            self.register_buffer('gamma_t', torch.tensor(config.gamma))&#10;            self.encoder = nn.Parameter(torch.randn(config.d_model, n_u))&#10;            self.decoder = nn.Parameter(torch.randn(n_y, config.d_model))&#10;        else:&#10;            # All other cases (no γ or trainable γ): simple Linear layers&#10;            self.encoder = nn.Linear(n_u, config.d_model, bias=False)&#10;            self.decoder = nn.Linear(config.d_model, n_y, bias=False)&#10;&#10;        self.blocks = nn.ModuleList([SSL(config) for _ in range(config.n_layers)])&#10;&#10;    def forward(self, u, state=None, gamma=None, mode=&quot;loop&quot;):&#10;        &quot;&quot;&quot;&#10;            Initial pre-processing common to all methods.&#10;        &quot;&quot;&quot;&#10;        batch_size, seq_len, input_dim = u.shape&#10;&#10;        # Initialize states for all layers if not provided&#10;        if state is None:&#10;            layer_states = [None] * len(self.blocks)&#10;        else:&#10;            layer_states = state if isinstance(state, list) else [state] * len(self.blocks)&#10;&#10;        # Pre-allocate output tensor with correct dimensions&#10;        if isinstance(self.decoder, nn.Linear):&#10;            output_dim = self.decoder.out_features&#10;        else:&#10;            output_dim = self.decoder.shape[0]&#10;&#10;        # Pre-allocate outputs for better memory efficiency&#10;        outputs = torch.empty(batch_size, seq_len, output_dim, device=u.device, dtype=u.dtype)&#10;&#10;        # Process encoder once for the entire sequence&#10;        if isinstance(self.encoder, nn.Linear):&#10;            x = self.encoder(u)  # (B, L, d_model)&#10;        else:&#10;            x = u @ self.encoder.T&#10;&#10;        if mode.startswith(&quot;loop&quot;):&#10;            &quot;&quot;&quot;&#10;            Efficient sequential processing: single loop over time, passing through all layers at each timestep.&#10;            &quot;&quot;&quot;&#10;            # Create a list to store the output of each timestep&#10;            processed_timesteps = []&#10;&#10;            # Single optimized loop over time&#10;            for t in range(seq_len):&#10;                x_t = x[:, t, :]  # Current timestep: (B, d_model)&#10;&#10;                # Optimized layer processing&#10;                for layer_idx, block in enumerate(self.blocks):&#10;                    # Reshape for SSL forward: (B, 1, d_model)&#10;                    x_t_expanded = x_t.unsqueeze(1)&#10;&#10;                    # Pass through the SSL block&#10;                    x_t_out, st = block(x_t_expanded, state=layer_states[layer_idx], mode=&quot;loop&quot;)&#10;&#10;                    # Squeeze back to (B, d_model) and update the state&#10;                    x_t = x_t_out.squeeze(1)&#10;                    layer_states[layer_idx] = st&#10;&#10;                processed_timesteps.append(x_t)&#10;&#10;            # Stack all timesteps and decode the entire sequence at once&#10;            processed_sequence = torch.stack(processed_timesteps, dim=1)&#10;&#10;            if self.config.robust is True and self.config.gamma is not None:&#10;                &quot;&quot;&quot;&#10;                This is the case where we use a fixed gamma for LRU_Robust.&#10;                &quot;&quot;&quot;&#10;                # Handle the fixed gamma case for LRU_Robust&#10;                gamma_t = torch.abs(self.gamma_t) if gamma is None else gamma&#10;                gammaLRU = [torch.abs(block.lru.gamma) for block in self.blocks]&#10;                gammaLRU_tensor = torch.stack(gammaLRU)&#10;                encoder_norm = torch.norm(self.encoder, 2)&#10;                decoder_norm = torch.norm(self.decoder, 2)&#10;                gamma_prod = torch.prod(gammaLRU_tensor) + 1&#10;                decoder_scaled = (gamma_t * self.decoder) / (encoder_norm * decoder_norm * gamma_prod)&#10;                outputs = processed_sequence @ decoder_scaled.T&#10;            else:&#10;                if isinstance(self.decoder, nn.Linear):&#10;                    outputs = self.decoder(processed_sequence)&#10;                else:&#10;                    outputs = processed_sequence @ self.decoder.T&#10;        else:&#10;            &quot;&quot;&quot;&#10;               This is the case where we use parallel scan modes.&#10;            &quot;&quot;&quot;&#10;            # Standard processing for regular LRU for scan modes&#10;            for layer, block in enumerate(self.blocks):&#10;                x, st = block(x, mode=mode)&#10;                layer_states[layer] = st&#10;            outputs = self.decoder(x)&#10;&#10;&#10;        return outputs, layer_states&#10;&#10;&#10;    def reset(self):&#10;        # Reset initial states of LTI systems&#10;        for block in self.blocks:&#10;            block.lru.reset()&#10;&#10;    # setters and getters&#10;    def get_parameter_shapes(self):&#10;        param_dict = OrderedDict(&#10;            (name, getattr(self, name).shape) for name in self.training_param_names&#10;        )&#10;        return param_dict&#10;&#10;    def get_named_parameters(self):&#10;        param_dict = OrderedDict(&#10;            (name, getattr(self, name)) for name in self.training_param_names&#10;        )&#10;        return param_dict&#10;" />
              <option name="updatedContent" value="import math&#10;from dataclasses import dataclass&#10;import torch&#10;import torch.nn as nn&#10;from collections import OrderedDict&#10;from lru.scan_utils import associative_scan, binary_operator_diag&#10;import torch.jit as jit&#10;from lru.L_bounded_MLPs import FirstChannel, SandwichFc, SandwichLin&#10;from typing import Optional&#10;&#10;&#10;&quot;&quot;&quot; Linear Recurrent Units ----------------------------------------- &quot;&quot;&quot;&#10;&#10;class LRU(nn.Module):&#10;    &quot;&quot;&quot; Linear Recurrent Unit. The LRU is simulated using Parallel Scan (fast!) when&#10;     &quot;scan&quot; is set to True (default) in the forward pass, otherwise recursively (slow).&quot;&quot;&quot;&#10;&#10;    def __init__(&#10;            self, in_features: int, out_features: int, state_features: int, internal_state_init=None, rmin=0.9,&#10;            rmax=1.0, max_phase=6.283&#10;    ):&#10;        super().__init__()&#10;        self.in_features = in_features&#10;        self.out_features = out_features&#10;        self.state_features = state_features&#10;&#10;        # Pre-compute constants for efficiency&#10;        self._sqrt_in_features = math.sqrt(in_features)&#10;        self._sqrt_2_in_features = math.sqrt(2 * in_features)&#10;        self._sqrt_state_features = math.sqrt(state_features)&#10;        self._rmin_rmax_diff = rmax - rmin&#10;        self._rmin_rmax_sum = rmax + rmin&#10;        self._rmin_squared = rmin ** 2&#10;&#10;        self.D = nn.Parameter(&#10;            torch.randn([out_features, in_features]) / self._sqrt_in_features&#10;        )&#10;&#10;        u1 = torch.rand(state_features)&#10;        u2 = torch.rand(state_features)&#10;        self.nu_log = nn.Parameter(&#10;            torch.log(-0.5 * torch.log(u1 * self._rmin_rmax_sum * self._rmin_rmax_diff + self._rmin_squared))&#10;        )&#10;        self.theta_log = nn.Parameter(torch.log(max_phase * u2))&#10;&#10;        lambda_abs = torch.exp(-torch.exp(self.nu_log))&#10;        self.gamma_log = nn.Parameter(&#10;            torch.log(torch.sqrt(1.0 - lambda_abs.square()))  # More efficient than torch.ones_like and torch.square&#10;        )&#10;&#10;        # More efficient initialization using a single complex tensor creation&#10;        B_complex = torch.complex(&#10;            torch.randn([state_features, in_features]) / self._sqrt_2_in_features,&#10;            torch.randn([state_features, in_features]) / self._sqrt_2_in_features&#10;        )&#10;        self.B = nn.Parameter(B_complex)  # N, U&#10;&#10;        C_complex = torch.complex(&#10;            torch.randn([out_features, state_features]) / self._sqrt_state_features,&#10;            torch.randn([out_features, state_features]) / self._sqrt_state_features&#10;        )&#10;        self.C = nn.Parameter(C_complex)  # H, N&#10;&#10;        # initialize internal state&#10;        self.state = None&#10;&#10;        # Pre-compute transformation matrices for ss_real_matrices method&#10;        self._T_block = None&#10;        self._T_block_inv = None&#10;&#10;    def ss_params(self):&#10;        lambda_abs = torch.exp(-torch.exp(self.nu_log))&#10;        lambda_phase = torch.exp(self.theta_log)&#10;&#10;        # More efficient complex number creation&#10;        lambdas = lambda_abs * torch.exp(1j * lambda_phase)&#10;        gammas = torch.exp(self.gamma_log).unsqueeze(-1)&#10;        B = gammas * self.B&#10;        return lambdas, B, self.C, self.D&#10;&#10;    def ss_real_matrices(self, to_numpy=True):&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # Pre-allocate with the correct dtype and device&#10;        device, dtype = lambdas.device, lambdas.dtype&#10;        state_features_2 = 2 * self.state_features&#10;&#10;        # More efficient tensor creation using stack instead of manual indexing&#10;        lambdas_conjugate = torch.stack([lambdas, lambdas.conj()], dim=1).flatten()&#10;        A_full = torch.diag(lambdas_conjugate)&#10;&#10;        # More efficient B_full creation&#10;        B_conjugate = torch.stack([B, B.conj()], dim=1).view(state_features_2, self.in_features)&#10;&#10;        # More efficient C_full creation&#10;        C_half = 0.5 * C&#10;        C_conjugate = torch.stack([C_half, C_half.conj()], dim=2).view(self.out_features, state_features_2)&#10;&#10;        # Cache transformation matrices&#10;        if self._T_block is None or self._T_block.device != device:&#10;            self._T_block = torch.tensor([[1, 1], [1j, -1j]], device=device, dtype=dtype)&#10;            self._T_block_inv = torch.linalg.inv(self._T_block)&#10;&#10;        T_full = torch.block_diag(*([self._T_block] * self.state_features))&#10;        T_full_inv = torch.block_diag(*([self._T_block_inv] * self.state_features))&#10;&#10;        # More efficient matrix operations using @ operator consistently&#10;        A_real = (T_full @ A_full @ T_full_inv).real&#10;        B_real = (T_full @ B_conjugate).real&#10;        C_real = (C_conjugate @ T_full_inv).real&#10;        D_real = D&#10;&#10;        ss_real_params = [A_real, B_real, C_real, D_real]&#10;        if to_numpy:&#10;            ss_real_params = [param.detach().cpu().numpy() for param in ss_real_params]&#10;&#10;        return tuple(ss_real_params)&#10;&#10;    def forward_loop(self, input, state=None):&#10;        batch_size, seq_len, _ = input.shape&#10;&#10;        # State management&#10;        if self.state is None or self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                     device=input.device, dtype=torch.complex64)&#10;&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # State computation using pre-converted input&#10;        input_B_dtype = input.to(B.dtype)&#10;        B_T = B.mT  # Cache transpose&#10;&#10;        # Optimized loop with pre-allocated tensor for states&#10;        inner_states = torch.empty(batch_size, seq_len, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        # Vectorized state updates&#10;        current_state = self.state&#10;        for t, u_step in enumerate(input_B_dtype.unbind(dim=1)):&#10;            inner_states[:, t] = current_state&#10;            current_state = lambdas * current_state + u_step @ B_T&#10;&#10;        self.state = current_state  # Update the internal state&#10;&#10;        # Output computation using all inner states&#10;        output = (inner_states @ C.mT).real + input @ D.T&#10;&#10;        return output, inner_states&#10;&#10;    @torch.compiler.disable&#10;    def forward_scan(self, input, state=None):&#10;        &quot;&quot;&quot;&#10;        Computes the LRU output using a parallel scan.&#10;&#10;        Args:&#10;            input (torch.Tensor): (B, L, H) input sequence.&#10;            state (torch.Tensor, optional): (B, N) initial state. If None, a zero state is used.&#10;&#10;        Returns:&#10;            Tuple[torch.Tensor, torch.Tensor]:&#10;                - (B, L, H_out) output sequence.&#10;                - (B, L, N) sequence of internal states.&#10;        &quot;&quot;&quot;&#10;        batch_size, seq_len, _ = input.shape&#10;        lambdas, B, C, D = self.ss_params()&#10;&#10;        # If no initial state is provided, initialize it to zeros.&#10;        if self.state is None or self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        # Pre-compute input transformation&#10;        Bu_elements = input.to(B.dtype) @ B.mT&#10;&#10;        # Incorporate the initial state into the first element of the sequence&#10;        Bu_elements[:, 0, :] += lambdas * self.state&#10;&#10;        # Define the scan function for vmap&#10;        lambda_elements = lambdas.expand(seq_len, -1)&#10;&#10;        def scan_fn(Bu_seq):&#10;            return associative_scan(binary_operator_diag, (lambda_elements, Bu_seq))[1]&#10;&#10;        # Apply the scan over the batch dimension&#10;        scanned_states = torch.vmap(scan_fn)(Bu_elements)&#10;&#10;        # Prepend the initial state to get the full state sequence&#10;        inner_states = torch.cat([self.state.unsqueeze(1), scanned_states[:, :-1, :]], dim=1)&#10;&#10;        # Update the internal state of the LRU module to the last state&#10;        self.state = scanned_states[:, -1, :]&#10;&#10;        # Compute the final output&#10;        output = (inner_states @ C.mT).real + input @ D.T&#10;        return output, inner_states&#10;&#10;    def forward(self, input, gamma=None, state=None, mode=&quot;loop&quot;):&#10;        if mode == &quot;scan&quot;:&#10;            return self.forward_scan(input, self.state)&#10;        elif mode in [&quot;loop&quot;, &quot;loop_efficient&quot;]:&#10;            return self.forward_loop(input, self.state)&#10;        else:&#10;            raise ValueError(f&quot;Unknown mode: {mode}. Expected 'scan', 'loop', or 'loop_efficient'.&quot;)&#10;&#10;&#10;    def reset(self):&#10;        self.state = None  # reset the SSM state to the initial value&#10;&#10;&#10;# WORK IN PROGRESS&#10;&#10;class LRU_Robust(jit.ScriptModule):&#10;    &quot;&quot;&quot; Implements a Linear Recurrent Unit (LRU) with trainable or prescribed l2 gain gamma.&#10;    No parallel scan implementation is available at the moment. &quot;&quot;&quot;&#10;&#10;    def __init__(self, state_features: int, gamma: float):&#10;        super().__init__()&#10;        self.state_features = state_features&#10;        self.gamma = gamma&#10;        if gamma is None:&#10;            self.gamma = nn.Parameter(torch.tensor(33.0))&#10;        # initialize the internal state with TorchScript-friendly Optional[Tensor]&#10;        self.state = torch.zeros(1)&#10;        self.register_buffer('ID', torch.eye(state_features))&#10;&#10;        self.alpha = nn.Parameter(torch.tensor(4.1))  # controls the initialization of the matrix A:&#10;        # the larger the alpha at initialization, the closer the eigenvalues of A will be&#10;        # to the boundary of the unitary circle at initialization. This helps the SSM to obtain long memory properties.&#10;&#10;        self.epsilon = nn.Parameter(torch.tensor(-99.9))  # Regularization&#10;&#10;        self.Skew = nn.Parameter(0.01 * torch.randn(state_features, state_features))&#10;&#10;        # Define each block of X as a parameter&#10;        self.X11 = nn.Parameter(torch.eye(state_features))&#10;        self.X22 = nn.Parameter(0.01 * torch.eye(state_features))&#10;        self.X21 = nn.Parameter(torch.eye(state_features))&#10;&#10;        self.C = nn.Parameter(torch.eye(state_features))&#10;        self.D = nn.Parameter(torch.eye(state_features))&#10;&#10;        # self.X11 = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.X22 = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.X21 = nn.Parameter(torch.randn(state_features, state_features))&#10;        #&#10;        # self.C = nn.Parameter(torch.randn(state_features, state_features))&#10;        # self.D = nn.Parameter(torch.randn(state_features, state_features))&#10;&#10;    @jit.script_method&#10;    def set_param(self):  # Parameter update for l2 gain (free param)&#10;&#10;        gamma = self.gamma&#10;        # Auxiliary Parameters&#10;        X11 = self.X11&#10;        X22 = self.X22&#10;&#10;        Sk = self.Skew - self.Skew.T&#10;        Q = (self.ID - Sk) @ torch.linalg.inv(self.ID + Sk)&#10;        Z = self.X21 @ self.X21.T + X22 @ X22.T + self.D.T @ self.D + torch.exp(self.epsilon) * self.ID&#10;        beta = gamma ** 2 * torch.sigmoid(self.alpha) / torch.norm(Z, 2)&#10;        H11 = X11 @ X11.T + self.C.T @ self.C + beta * torch.exp(self.epsilon) * self.ID&#10;        H12 = torch.sqrt(beta) * (X11 @ self.X21.T + self.C.T @ self.D)&#10;        V = Z * beta - gamma ** 2 * self.ID&#10;        R = H12 @ torch.linalg.inv(V.T) @ H12.T&#10;        CR = torch.linalg.cholesky(-R)&#10;        CRH = torch.linalg.cholesky(-R + H11)&#10;&#10;        # LTI system matrices&#10;&#10;        A = torch.linalg.inv(CRH).T @ Q @ CR.T&#10;        B = A @ torch.linalg.inv(H12.T) @ V.T&#10;        C = self.C&#10;        D = torch.sqrt(beta) * self.D&#10;&#10;        return A, B, C, D&#10;&#10;    #state: Optional[torch.Tensor]&#10;    @jit.script_method&#10;    def forward(self, input: torch.Tensor, mode: str = &quot;loop&quot;) -&gt; tuple[torch.Tensor, torch.Tensor]:&#10;        # Input size: (B, L, H)&#10;        batch_size = input.shape[0]&#10;&#10;        # Initialize state if needed&#10;        if self.state.shape[0] != batch_size:&#10;            self.state = torch.zeros(batch_size, self.state_features,&#10;                                   device=input.device, dtype=torch.complex64)&#10;&#10;        A, B, C, D = self.set_param()&#10;        if input.dim() == 1:&#10;            input = input.unsqueeze(0).unsqueeze(0)&#10;        # output = torch.empty(&#10;        #     [i for i in input.shape[:-1]] + [self.state_features], device=self.C.device&#10;        # )&#10;&#10;        states = []&#10;&#10;        # LTI dynamics&#10;        for u_step in input.split(1, dim=1):  # 1 is the time dimension&#10;            u_step = u_step.squeeze(1)&#10;            self.state = self.state @ A.T + u_step @ B.T&#10;            states.append(self.state)&#10;&#10;        states = torch.stack(states, 1)&#10;        output = states @ C.mT + input @ D.T&#10;        return output, states&#10;&#10;    def reset(self):&#10;        self.state = None  # reset the LRU state to the initial value&#10;&#10;&quot;&quot;&quot; SSM models ----------------------------------------- &quot;&quot;&quot;&#10;&#10;&quot;&quot;&quot; Data class to set up the SSM model (values here are used just to initialize all fields) &quot;&quot;&quot;&#10;@dataclass&#10;class SSMConfig:&#10;    d_model: int = 10  # input/output size of the LRU (n_u = n_y)&#10;    d_state: int = 64  # state size of the LRU (n_x)&#10;    n_layers: int = 6  # number of SSMs blocks in cascade for deep structures&#10;    dropout: float = 0.0  # set it different from 0 if you want to introduce dropout regularization&#10;    bias: bool = False  # bias of MLP layers&#10;    rmin: float = 0.0  # min. magnitude of the eigenvalues at initialization in the complex parametrization&#10;    rmax: float = 1.0  # max. magnitude of the eigenvalues at initialization in the complex parametrization&#10;    max_phase: float = 2 * math.pi  # maximum phase of the eigenvalues at initialization in the complex parametrization&#10;    ff: str = &quot;MLP&quot;  # non-linear block used in the scaffolding&#10;    scale: float = 1  # Lipschitz constant of the Lipschitz bounded MLP (LMLP)&#10;    dim_amp: int = 4  # controls the hidden layer's dimension of the MLP&#10;    robust: bool = True  # set this to true if you want to use the l2 gain parametrization for the SSM. If set to false,&#10;    # the complex diagonal parametrization of the LRU will be used instead.&#10;    gamma: float = None  # set the overall l2 gain value in case you want to keep it fixed and not trainable, if set to&#10;    # None, the gain will be trainable.&#10;&#10;&#10;    # Parallel scan must be selected in the forward call. It will be disabled when gamma is set to True.&#10;&#10;    &quot;&quot;&quot; Scaffolding Layers &quot;&quot;&quot;&#10;&#10;class MLP(nn.Module):&#10;    &quot;&quot;&quot; Standard Transformer MLP &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        # Pre-compute hidden dimension for efficiency&#10;        self.hidden_dim = config.dim_amp * config.d_model&#10;&#10;        self.c_fc = nn.Linear(config.d_model, self.hidden_dim, bias=False)&#10;        self.gelu = nn.GELU()&#10;        self.c_proj = nn.Linear(self.hidden_dim, config.d_model, bias=False)&#10;        self.dropout = nn.Dropout(config.dropout) if config.dropout &gt; 0 else nn.Identity()&#10;&#10;    def forward(self, x):&#10;        x = self.c_fc(x)&#10;        x = self.gelu(x)&#10;        x = self.c_proj(x)&#10;        return self.dropout(x)&#10;&#10;&#10;class LMLP(nn.Module):&#10;    &quot;&quot;&quot; Implements a Lipschitz.-bounded MLP with sandwich layers. The square root&#10;    # of the Lipschitz bound is given by the scale parameter &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        # Pre-compute hidden dimension for efficiency&#10;        hidden_dim = config.dim_amp * config.d_model&#10;&#10;        # More efficient layer construction using list comprehension&#10;        layers = [&#10;            FirstChannel(config.d_model, scale=config.scale),&#10;            SandwichFc(config.d_model, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichFc(hidden_dim, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichFc(hidden_dim, hidden_dim, bias=False, scale=config.scale),&#10;            SandwichLin(hidden_dim, config.d_model, bias=False, scale=config.scale)&#10;        ]&#10;&#10;        # Only add dropout if needed&#10;        if config.dropout &gt; 0:&#10;            layers.append(nn.Dropout(config.dropout))&#10;&#10;        self.model = nn.Sequential(*layers)&#10;&#10;    def forward(self, input):&#10;        return self.model(input)&#10;&#10;&#10;class GLU(nn.Module):&#10;    &quot;&quot;&quot; The static non-linearity used in the S4 paper &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        self.activation = nn.GELU()&#10;        self.dropout = nn.Dropout(config.dropout) if config.dropout &gt; 0 else nn.Identity()&#10;&#10;        # More efficient sequential construction&#10;        self.output_linear = nn.Sequential(&#10;            nn.Linear(config.d_model, 2 * config.d_model),&#10;            nn.GLU(dim=-1),&#10;        )&#10;&#10;    def forward(self, x):&#10;        x = self.dropout(self.activation(x))&#10;        return self.output_linear(x)&#10;&#10;    &quot;&quot;&quot; SSMs blocks ----------------------------------------- &quot;&quot;&quot;&#10;&#10;class SSL(nn.Module):&#10;    &quot;&quot;&quot; State Space Layer: LRU --&gt; MLP + skip connection &quot;&quot;&quot;&#10;&#10;    def __init__(self, config: SSMConfig):&#10;        super().__init__()&#10;        self.ln = nn.LayerNorm(config.d_model, bias=config.bias)&#10;&#10;        # More efficient LRU initialization&#10;        if config.robust:&#10;            self.lru = LRU_Robust(config.d_model, config.gamma)&#10;        else:&#10;            self.lru = LRU(config.d_model, config.d_model, config.d_state,&#10;                           rmin=config.rmin, rmax=config.rmax, max_phase=config.max_phase)&#10;&#10;        # Dictionary for layer selection&#10;        ff_layers = {&#10;            &quot;GLU&quot;: lambda: GLU(config),&#10;            &quot;MLP&quot;: lambda: MLP(config),&#10;            &quot;LMLP&quot;: lambda: LMLP(config)&#10;        }&#10;&#10;        if config.ff not in ff_layers:&#10;            raise ValueError(f&quot;Unknown feedforward type: {config.ff}&quot;)&#10;&#10;        self.ff = ff_layers[config.ff]()&#10;        self.dropout = nn.Dropout(config.dropout)&#10;&#10;    def forward(self, x, state=None, mode: str = &quot;loop&quot;):&#10;        z = x&#10;        # z = self.ln(z)  # prenorm&#10;&#10;        z, st = self.lru(z, state=state, mode=mode)&#10;        z = self.ff(z)  # MLP, GLU or LMLP&#10;        z = self.dropout(z)&#10;&#10;        # Residual connection&#10;        return z + x, st&#10;&#10;&#10;class DeepSSM(nn.Module):&#10;    &quot;&quot;&quot; Deep SSM: encoder --&gt; cascade of n SSM blocks --&gt; decoder  &quot;&quot;&quot;&#10;&#10;    def __init__(self, n_u: int, n_y: int, config: SSMConfig):&#10;        super().__init__()&#10;&#10;        self.config = config&#10;&#10;        # Simplified initialization - only handle trainable gamma for LRU_Robust&#10;        if config.robust is True and config.gamma is not None:&#10;            # Fixed-γ: register buffer and use raw Parameters&#10;            self.register_buffer('gamma_t', torch.tensor(config.gamma))&#10;            self.encoder = nn.Parameter(torch.randn(config.d_model, n_u))&#10;            self.decoder = nn.Parameter(torch.randn(n_y, config.d_model))&#10;        else:&#10;            # All other cases (no γ or trainable γ): simple Linear layers&#10;            self.encoder = nn.Linear(n_u, config.d_model, bias=False)&#10;            self.decoder = nn.Linear(config.d_model, n_y, bias=False)&#10;&#10;        self.blocks = nn.ModuleList([SSL(config) for _ in range(config.n_layers)])&#10;&#10;    def forward(self, u, state=None, gamma=None, mode=&quot;loop&quot;):&#10;        &quot;&quot;&quot;&#10;            Initial pre-processing common to all methods.&#10;        &quot;&quot;&quot;&#10;        batch_size, seq_len, input_dim = u.shape&#10;&#10;        # Initialize states for all layers if not provided&#10;        if state is None:&#10;            layer_states = [None] * len(self.blocks)&#10;        else:&#10;            layer_states = state if isinstance(state, list) else [state] * len(self.blocks)&#10;&#10;        # Pre-allocate output tensor with correct dimensions&#10;        if isinstance(self.decoder, nn.Linear):&#10;            output_dim = self.decoder.out_features&#10;        else:&#10;            output_dim = self.decoder.shape[0]&#10;&#10;        # Pre-allocate outputs for better memory efficiency&#10;        outputs = torch.empty(batch_size, seq_len, output_dim, device=u.device, dtype=u.dtype)&#10;&#10;        # Process encoder once for the entire sequence&#10;        if isinstance(self.encoder, nn.Linear):&#10;            x = self.encoder(u)  # (B, L, d_model)&#10;        else:&#10;            x = u @ self.encoder.T&#10;&#10;        if mode.startswith(&quot;loop&quot;):&#10;            &quot;&quot;&quot;&#10;            Efficient sequential processing: single loop over time, passing through all layers at each timestep.&#10;            &quot;&quot;&quot;&#10;            # Create a list to store the output of each timestep&#10;            processed_timesteps = []&#10;&#10;            # Single optimized loop over time&#10;            for t in range(seq_len):&#10;                x_t = x[:, t, :]  # Current timestep: (B, d_model)&#10;&#10;                # Optimized layer processing&#10;                for layer_idx, block in enumerate(self.blocks):&#10;                    # Reshape for SSL forward: (B, 1, d_model)&#10;                    x_t_expanded = x_t.unsqueeze(1)&#10;&#10;                    # Pass through the SSL block&#10;                    x_t_out, st = block(x_t_expanded, state=layer_states[layer_idx], mode=&quot;loop&quot;)&#10;&#10;                    # Squeeze back to (B, d_model) and update the state&#10;                    x_t = x_t_out.squeeze(1)&#10;                    layer_states[layer_idx] = st&#10;&#10;                processed_timesteps.append(x_t)&#10;&#10;            # Stack all timesteps and decode the entire sequence at once&#10;            processed_sequence = torch.stack(processed_timesteps, dim=1)&#10;&#10;            if self.config.robust is True and self.config.gamma is not None:&#10;                &quot;&quot;&quot;&#10;                This is the case where we use a fixed gamma for LRU_Robust.&#10;                &quot;&quot;&quot;&#10;                # Handle the fixed gamma case for LRU_Robust&#10;                gamma_t = torch.abs(self.gamma_t) if gamma is None else gamma&#10;                gammaLRU = [torch.abs(block.lru.gamma) for block in self.blocks]&#10;                gammaLRU_tensor = torch.stack(gammaLRU)&#10;                encoder_norm = torch.norm(self.encoder, 2)&#10;                decoder_norm = torch.norm(self.decoder, 2)&#10;                gamma_prod = torch.prod(gammaLRU_tensor) + 1&#10;                decoder_scaled = (gamma_t * self.decoder) / (encoder_norm * decoder_norm * gamma_prod)&#10;                outputs = processed_sequence @ decoder_scaled.T&#10;            else:&#10;                if isinstance(self.decoder, nn.Linear):&#10;                    outputs = self.decoder(processed_sequence)&#10;                else:&#10;                    outputs = processed_sequence @ self.decoder.T&#10;        else:&#10;            &quot;&quot;&quot;&#10;               This is the case where we use parallel scan modes.&#10;            &quot;&quot;&quot;&#10;            # Standard processing for regular LRU for scan modes&#10;            for layer, block in enumerate(self.blocks):&#10;                x, st = block(x, mode=mode)&#10;                layer_states[layer] = st&#10;            outputs = self.decoder(x)&#10;&#10;&#10;        return outputs, layer_states&#10;&#10;&#10;    def reset(self):&#10;        # Reset initial states of LTI systems&#10;        for block in self.blocks:&#10;            block.lru.reset()&#10;&#10;    # setters and getters&#10;    def get_parameter_shapes(self):&#10;        param_dict = OrderedDict(&#10;            (name, getattr(self, name).shape) for name in self.training_param_names&#10;        )&#10;        return param_dict&#10;&#10;    def get_named_parameters(self):&#10;        param_dict = OrderedDict(&#10;            (name, getattr(self, name)) for name in self.training_param_names&#10;        )&#10;        return param_dict" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>